---
title: "p8105_hw3_yw3095"
author: "Yixuan Wang"
date: "October 11, 2018"
output: github_document
---

```{r setup, include=FALSE}
getwd()
devtools::install_github("thomasp85/patchwork")
library(tidyverse)
library(patchwork)
knitr::opts_chunk$set(echo = TRUE,
                      fig.width = 6,
                      fig.asp = .6,
                      out.width = "90%")
theme_set(theme_bw() + theme(legend.position = "bottom"))
```
##Problem 1
This problem uses the BRFSS data.

* Upload the dataset from the p8105.datasets package

```{r, message=FALSE}
devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
```
*   Read and clean the data 
```{r}
hw3_brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health", !is.na(response)) %>% 
  separate(locationdesc, into = c("remove", "county"), sep = " - ") %>% 
  rename(state = locationabbr) %>%
  select(-remove) %>% 
  mutate(response = as.factor(response), 
         response = ordered(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor")))
hw3_brfss
```
**Questions**

```{r}
hw3_brfss %>% 
  filter(year == 2002) %>%
  group_by(state) %>%  
  summarize(n = n_distinct(county)) %>% 
  filter(n == 7)
```

* In 2002, which states were observed at 7 locations?

   * States CT, FL, and NC were observed at 7 locations.
   
* Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

????????? the readability of your embedded plots (e.g. font sizes, axis labels, titles) 
```{r}
hw3_brfss %>% 
  group_by(year, state) %>%  
  summarize(n = n_distinct(county)) %>% 
  ggplot(aes(x = year, y = n)) +
  geom_line(aes(color = state)) + 
  labs(
    title = "Number of locations in states",
    x = "Year",
    y = "Number of locations",
    caption = "Data from the p8105.datasets package"
  ) + 
  viridis::scale_color_viridis(
    discrete = TRUE)

```
* Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

????group by year????
```{r}
hw3_brfss %>% 
  spread(key = response, value = data_value) %>%
  janitor::clean_names() %>%
  filter(state == "NY" & year == 2002 | year == 2006 | year == 2010, !is.na(excellent)) %>%
  group_by(county) %>%  
  summarize(mean_excellent = mean(excellent),
            sd_excellent = sd(excellent)) %>%  
knitr::kable(digits = 1)
```

* For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r}
excellent_plot = spread(hw3_brfss, key = response, value = data_value) %>%
  janitor::clean_names() %>%
  filter(!is.na(excellent)) %>%
  group_by(year, state) %>% 
  summarize(mean_excellent = mean(excellent)) %>% 
  ggplot(aes(x = year, y = mean_excellent, color = state)) +
  geom_line(aes(color = state), alpha = .5) +
  theme(legend.position = "none")

very_good_plot = spread(hw3_brfss, key = response, value = data_value) %>%
  janitor::clean_names() %>%
  filter(!is.na(very_good)) %>%
  group_by(year, state) %>% 
  summarize(mean_very_good = mean(very_good)) %>% 
  ggplot(aes(x = year, y = mean_very_good, color = state)) +
  geom_line(aes(color = state), alpha = .5) +
  theme(legend.position = "none")

good_plot = spread(hw3_brfss, key = response, value = data_value) %>%
  janitor::clean_names() %>%
  filter(!is.na(good)) %>%
  group_by(year, state) %>% 
  summarize(mean_good = mean(good)) %>% 
  ggplot(aes(x = year, y = mean_good, color = state)) +
  geom_line(aes(color = state), alpha = .5) +
  theme(legend.position = "none")

fair_plot = spread(hw3_brfss, key = response, value = data_value) %>%
  janitor::clean_names() %>%
  filter(!is.na(fair)) %>%
  group_by(year, state) %>% 
  summarize(mean_fair = mean(fair)) %>% 
  ggplot(aes(x = year, y = mean_fair, color = state)) +
  geom_line(aes(color = state), alpha = .5) +
  theme(legend.position = "none")

poor_plot = spread(hw3_brfss, key = response, value = data_value) %>%
  janitor::clean_names() %>%
  filter(!is.na(poor)) %>%
  group_by(year, state) %>% 
  summarize(mean_poor = mean(poor)) %>% 
  ggplot(aes(x = year, y = mean_poor, color = state)) +
  geom_line(aes(color = state), alpha = .5) +
  theme(legend.position = "bottom")


(excellent_plot + very_good_plot + good_plot) / (fair_plot + poor_plot)
```

##Problem 2
This problem uses the Instacart data.

*   Read and clean the data  
```{r}
hw3_insta = instacart %>% 
  janitor::clean_names()
hw3_insta
```
**Description of the dataset**

There are `r nrow(hw3_insta)` observations and `r ncol(hw3_insta)` variables in the instacart dataset.The key variables are product_id, user_id and asile_id. 
???????describing some key variables, and giving illstrative examples of observations. ??????

**Questions**

```{r}
count(hw3_insta, aisle) %>% 
  arrange(-n) %>% 
  head(1)
```
* How many aisles are there, and which aisles are the most items ordered from?

    * There are `r count(distinct(hw3_insta, aisle_id))` aisles in the instacart dataset. Fresh vegetables is the most items ordered from, and there are 150609 orders from fresh vegetables.
    
* Make a plot that shows the number of items ordered in each aisle. 
```{r}
aisle_plot =  group_by(hw3_insta, aisle) %>%
  summarize(n = n()) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_boxplot()
aisle_plot  
```

* Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.  ??????the most popular item?????
```{r}
hw3_insta %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>%      
  group_by(aisle, product_name) %>%
  summarize(n = n()) %>% 
  arrange(-n, .by_group = TRUE)
```

* Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. ?????2x7??????
```{r}
hw3_insta %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%      
  group_by(product_name, order_dow) %>%
  summarize(mean_hour_of_day = mean(order_hour_of_day)) %>% 
knitr::kable(digits = 1)
```

##Problem 3
This problem uses the NY NOAA data.

*   Read and clean the data  
```{r}
hw3_ny_noaa = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(tmax = as.numeric(tmax),
         tmin = as.numeric(tmin),
         tmin = tmin / 10,
         tmax = tmax / 10)
hw3_ny_noaa
```

**Description of the dataset**

There are `r nrow(hw3_ny_noaa)` observations and `r ncol(hw3_ny_noaa)` variables in the instacart dataset.The key variables are product_id, user_id and asile_id. 
???????describing some key variables, and giving illstrative examples of observations. ??????

indicating the extent to which missing data is an issue. 

**Questions**

* For snowfall, what are the most commonly observed values? Why?

    * For snowfall, `r which.max(table(hw3_ny_noaa$snow))` are the most commenly observed values. why????????
    
* Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?
```{r}
hw3_ny_noaa %>% 
  filter(month == 1 | month == 7) %>% 
  group_by(id, year) %>% 
  summarise(mean_tmax = mean(tmax)) %>% 
  ggplot(aes(x = date, y = mean_tax)) +
  geom_point(aes(color = id, alpha = .5)) +
  facet_grid(~month) +
  viridis::scale_color_viridis(
    name = "Station", 
    discrete = TRUE
  )
```

* Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
```{r}
t_plot = select(hw3_ny_noaa, date, tmax, tmin) %>% 
  ggplot(aes(x = date, fill = observation)) +
  geom_density(alpha = .5) + 
  facet_grid(~name) + 
  viridis::scale_fill_viridis(discrete = TRUE)

```
```{r}
snow_plot = filter(hw3_ny_noaa, snow > 0 & snow < 100) %>% 
  ggplot(aes(x = snow, y = year)) +
  geom_density_ridges(scale = .85) 
snow_plot
```



